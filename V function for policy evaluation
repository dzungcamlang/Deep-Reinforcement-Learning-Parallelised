def first_visit_mc_v(policy, num_episodes, gamma=0.95, T_max=200):
    returns_sum =  np.zeros(11)
    returns_count =  np.zeros(11)

    # The final value function
    V = np.zeros(11)
    V_history = list()

    for e in range(1, num_episodes + 1):

        # Generate an episode.
        episode = []
        state = env.reset()
        for t in range(T_max):
            action = policy(state)
            next_state, reward, done, = env.step(state, action)
            episode.append((state, action, reward))
            if done:
                break
            state = next_state

        states_in_episode = set(x[0] for x in episode)
            first_occurence_idx = next(i for i, x in enumerate(episode) if x[0] == state)
            G = sum([x[2] * (gamma ** i) for i, x in enumerate(episode[first_occurence_idx:])])
            returns_sum[state] += G
            returns_count[state] += 1.0
            V[state] = returns_sum[state] / returns_count[state]
        V_history.append(V.copy())
        
    return V_history
